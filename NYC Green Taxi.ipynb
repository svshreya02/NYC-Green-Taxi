{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, requests, pickle\n",
    "from scipy.stats import skew\n",
    "from shapely.geometry import Point,Polygon,MultiPoint,MultiPolygon\n",
    "from scipy.stats import ttest_ind, f_oneway, lognorm, levy, skew, chisquare\n",
    "#import scipy.stats as st\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "from tabulate import tabulate #pretty print of tables. source: http://txt.arboreus.com/2013/03/13/pretty-print-tables-in-python.html\n",
    "from shapely.geometry import Point,Polygon,MultiPoint\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Green Taxi\n",
    "\n",
    "In this notebook, I will explore data on New York City Green Taxi of [september 2015](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). I will start with some warm up questions about the dataset. Later, I will build a model to predict the percentage tip a driver would exepect on each trip. The code is fully written in python with few additional open-source libraries easy to install. \n",
    "- [shapely](https://pypi.python.org/pypi/Shapely)\n",
    "- [scikit learn](http://scikit-learn.org/stable/)\n",
    "- [tabulate](http://txt.arboreus.com/2013/03/13/pretty-print-tables-in-python.html)\n",
    "\n",
    "In this analysis, some notion of statistics and hypothesis test are used but are very easy to follow. This [handbook of statistics](http://www.biostathandbook.com/index.html) can be used as a reference to explain basics.\n",
    "\n",
    "## Warm up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's first download the dataset and print out the its size***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Download the September 2015 dataset\n",
    "if os.path.exists('data_september_2015.csv'): # Check if the dataset is present on local disk and load it\n",
    "    data = pd.read_csv('data_september_2015.csv')\n",
    "else: # Download dataset if not available on disk\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv\"\n",
    "    data = pd.read_csv(url)\n",
    "    data.to_csv(url.split('/')[-1])\n",
    "\n",
    "# Print the size of the dataset\n",
    "print \"Number of rows:\", data.shape[0]\n",
    "print \"Number of columns: \", data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's have a look at the distribution of trip distance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# define the figure with 2 subplots\n",
    "fig,ax = plt.subplots(1,2,figsize = (15,4)) \n",
    "\n",
    "# histogram of the number of trip distance\n",
    "data.Trip_distance.hist(bins=30,ax=ax[0])\n",
    "ax[0].set_xlabel('Trip Distance (miles)')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title('Histogram of Trip Distance with outliers included')\n",
    "\n",
    "# create a vector to contain Trip Distance\n",
    "v = data.Trip_distance \n",
    "# exclude any data point located further than 3 standard deviations of the median point and \n",
    "# plot the histogram with 30 bins\n",
    "v[~((v-v.median()).abs()>3*v.std())].hist(bins=30,ax=ax[1]) # \n",
    "ax[1].set_xlabel('Trip Distance (miles)')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('A. Histogram of Trip Distance (without outliers)')\n",
    "\n",
    "# apply a lognormal fit. Use the mean of trip distance as the scale parameter\n",
    "scatter,loc,mean = lognorm.fit(data.Trip_distance.values,\n",
    "                               scale=data.Trip_distance.mean(),\n",
    "                               loc=0)\n",
    "pdf_fitted = lognorm.pdf(np.arange(0,12,.1),scatter,loc,mean)\n",
    "ax[1].plot(np.arange(0,12,.1),600000*pdf_fitted,'r') \n",
    "ax[1].legend(['data','lognormal fit'])\n",
    "\n",
    "# export the figure\n",
    "plt.savefig('Question2.jpeg',format='jpeg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trip Distance is asymmetrically distributed. It is skewed to the right and it has a median smaller than its mean and both smaller than the standard deviation. The skewness is due to the fact that the variable has a lower boundary of 0. The distance can't be negative. [**This distribution has a structure of a lognormal distribution**](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm). To the left is plotted the distribution of the entire raw set of Trip distance. To the right, outliers have been removed before plotting. *Outliers are defined as any point located further than 3 standard deviations from the mean*\n",
    "\n",
    "**The hypothesis:** The trips are not random. If there were random, we would have a (symmetric) Gaussian distribution. The non-zero autocorrelation may be related the fact that people taking ride are pushed by a common cause, for instance, people rushing to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's see if the time of the day has any impact on the trip distance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# First, convert pickup and drop off datetime variable in their specific righ format\n",
    "data['Pickup_dt'] = data.lpep_pickup_datetime.apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "data['Dropoff_dt'] = data.Lpep_dropoff_datetime.apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Second, create a variable for pickup hours\n",
    "data['Pickup_hour'] = data.Pickup_dt.apply(lambda x:x.hour)\n",
    "\n",
    "# Mean and Median of trip distance by pickup hour\n",
    "# I will generate the table but also generate a plot for a better visualization\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(9,5)) # prepare fig to plot mean and median values\n",
    "# use a pivot table to aggregate Trip_distance by hour\n",
    "table1 = data.pivot_table(index='Pickup_hour', values='Trip_distance',aggfunc=('mean','median')).reset_index()\n",
    "# rename columns\n",
    "table1.columns = ['Hour','Mean_distance','Median_distance']\n",
    "table1[['Mean_distance','Median_distance']].plot(ax=ax)\n",
    "plt.ylabel('Metric (miles)')\n",
    "plt.xlabel('Hours after midnight')\n",
    "plt.title('Distribution of trip distance by pickup hour')\n",
    "#plt.xticks(np.arange(0,30,6)+0.35,range(0,30,6))\n",
    "plt.xlim([0,23])\n",
    "plt.savefig('Question3_1.jpeg',format='jpeg')\n",
    "plt.show()\n",
    "print '-----Trip distance by hour of the day-----\\n'\n",
    "print tabulate(table1.values.tolist(),[\"Hour\",\"Mean distance\",\"Median distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> We observe long range trips in the morning and evenings. Are these people commuting to work? If so how do they get back home. The evening peak are shorter than the morning peak. I would hypothesize that people are okay to take cabs in the morning to avoid being late to their early appointments while they would take public transportation in the evening. However, this might not apply to NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's also compare trips that originate (or terminate) from (at) one of the NYC airports. We can look at how many they are, the average fair, ...***\n",
    "\n",
    "Reading through the dictionary of variables, I found that the variable RateCodeID contains values indicating the final rate that was applied. Among those values, I realized that there is Newark and JFK which are the major airports in New York. In this part, I will use this knowledge and group data with RateCodeID 2 (JFK) and 3 (Newark). - An alternative (which I didn't due to time constraint) is to (1) get coordinates of airports from google map or http://transtats.bts.gov (2) get at least 4 points defining a rectangular buffer zone near the airport (3) build a polygon shape using shapely [https://pypi.python.org/pypi/Shapely] and (3) check if any pickup/dropoff location coordinates is within the polygon using shapely again. This method was first tried but was found to be time consuming -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# select airport trips\n",
    "airports_trips = data[(data.RateCodeID==2) | (data.RateCodeID==3)]\n",
    "print \"Number of trips to/from NYC airports: \", airports_trips.shape[0]\n",
    "print \"Average fare (calculated by the meter) of trips to/from NYC airports: $\", airports_trips.Fare_amount.mean(),\"per trip\"\n",
    "print \"Average total charged amount (before tip) of trips to/from NYC airports: $\", airports_trips.Total_amount.mean(),\"per trip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the number and mean fare of airport trips, let's have aso look at how trips are distributed by trip distances and hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# create a vector to contain Trip Distance for\n",
    "v2 = airports_trips.Trip_distance # airport trips\n",
    "v3 = data.loc[~data.index.isin(v2.index),'Trip_distance'] # non-airport trips\n",
    "\n",
    "# remove outliers: \n",
    "# exclude any data point located further than 3 standard deviations of the median point and \n",
    "# plot the histogram with 30 bins\n",
    "v2 = v2[~((v2-v2.median()).abs()>3*v2.std())]\n",
    "v3 = v3[~((v3-v3.median()).abs()>3*v3.std())] \n",
    "\n",
    "# define bins boundaries\n",
    "bins = np.histogram(v2,normed=True)[1]\n",
    "h2 = np.histogram(v2,bins=bins,normed=True)\n",
    "h3 = np.histogram(v3,bins=bins,normed=True)\n",
    "\n",
    "# plot distributions of trip distance normalized among groups\n",
    "fig,ax = plt.subplots(1,2,figsize = (15,4))\n",
    "w = .4*(bins[1]-bins[0])\n",
    "ax[0].bar(bins[:-1],h2[0],alpha=1,width=w,color='b')\n",
    "ax[0].bar(bins[:-1]+w,h3[0],alpha=1,width=w,color='g')\n",
    "ax[0].legend(['Airport trips','Non-airport trips'],loc='best',title='group')\n",
    "ax[0].set_xlabel('Trip distance (miles)')\n",
    "ax[0].set_ylabel('Group normalized trips count')\n",
    "ax[0].set_title('A. Trip distance distribution')\n",
    "#ax[0].set_yscale('log')\n",
    "\n",
    "# plot hourly distribution\n",
    "airports_trips.Pickup_hour.value_counts(normalize=True).sort_index().plot(ax=ax[1])\n",
    "data.loc[~data.index.isin(v2.index),'Pickup_hour'].value_counts(normalize=True).sort_index().plot(ax=ax[1])\n",
    "ax[1].set_xlabel('Hours after midnight')\n",
    "ax[1].set_ylabel('Group normalized trips count')\n",
    "ax[1].set_title('B. Hourly distribution of trips')\n",
    "ax[1].legend(['Airport trips','Non-airport trips'],loc='best',title='group')\n",
    "plt.savefig('Question3_2.jpeg',format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. The trip distance distribution shows two peaks. Airport trips follow the same trend as the rest of the trips for short trips (trip distance â‰¤ 2miles). However, there is also an increased number of long range trips (18 miles) which might correspond to a great number people coming to airports from further residential areas. A check on google map shows that the distance between JFK and Manhattan is about 18 miles whereas Newark to Manhattan is 15 miles.\n",
    "\n",
    "B. The hourly distribution shows that the number of trips at airports peaks around 3PM while it peaks 2 hours later. On the other hand, there is a shortage in airports riders at 2AM while the rest of NYC goes completely down 3 hours later 5AM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model\n",
    "In this section, I am going to guide my analysis towards building a model to predict the percentage tip\n",
    "\n",
    "***1. Let's build a derived variable for tip as a percentage of the total fare.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with this, some cleaning is necessary. \n",
    "Since the [initial charge for NYC green taxi is $2.5](http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml), any transaction with a smaller total amount  is invalid, thus it is to be dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data = data[(data.Total_amount>=2.5)] #cleaning\n",
    "data['Tip_percentage'] = 100*data.Tip_amount/data.Total_amount\n",
    "print \"Summary: Tip percentage\\n\",data.Tip_percentage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Similarly to the comparison between trips to/from airports with the rest of the trips, it is worthy to spend more time and check wether trips originating from upper manhattan have different percentage tip.***\n",
    "\n",
    "To identify trips originating from upper manhattan:\n",
    "- From googgle map, collect latitude and longitude data of at least 12 points that approximately define the bounding box of upper Manhattan \n",
    "- Create a polygon using shapely.geometry.Polygon [https://pypi.python.org/pypi/Shapely]\n",
    "- Check if the polygon contains  a location defined by (latitude,longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from shapely.geometry import Point,Polygon,MultiPoint\n",
    "# data points that define the bounding box of the Upper Manhattan\n",
    "umanhattan = [(40.796937, -73.949503),(40.787945, -73.955822),(40.782772, -73.943575),\n",
    "              (40.794715, -73.929801),(40.811261, -73.934153),(40.835371, -73.934515),\n",
    "              (40.868910, -73.911145),(40.872719, -73.910765),(40.878252, -73.926350),\n",
    "              (40.850557, -73.947262),(40.836225, -73.949899),(40.806050, -73.971255)]\n",
    "\n",
    "poi = Polygon(umanhattan)\n",
    "# create a function to check if a location is located inside Upper Manhattan\n",
    "def is_within_bbox(loc,poi=poi):\n",
    "    \"\"\"\n",
    "    This function returns 1 if a location loc(lat,lon) is located inside a polygon of interest poi\n",
    "    loc: tuple, (latitude, longitude)\n",
    "    poi: shapely.geometry.Polygon, polygon of interest\n",
    "    \"\"\"\n",
    "    return 1*(Point(loc).within(poi))\n",
    "tic = dt.datetime.now()\n",
    "# Create a new variable to check if a trip originated in Upper Manhattan\n",
    "data['U_manhattan'] = data[['Pickup_latitude','Pickup_longitude']].apply(lambda r:is_within_bbox((r[0],r[1])),axis=1)\n",
    "print \"Processing time \", dt.datetime.now()-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare distributions of the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# create a vector to contain Tip percentage for\n",
    "v1 = data[(data.U_manhattan==0) & (data.Tip_percentage>0)].Tip_percentage\n",
    "v2 = data[(data.U_manhattan==1) & (data.Tip_percentage>0)].Tip_percentage\n",
    "\n",
    "# generate bins and histogram values\n",
    "bins = np.histogram(v1,bins=10)[1]\n",
    "h1 = np.histogram(v1,bins=bins)\n",
    "h2 = np.histogram(v2,bins=bins)\n",
    "\n",
    "# generate the plot\n",
    "# First suplot: visualize all data with outliers\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "w = .4*(bins[1]-bins[0])\n",
    "ax.bar(bins[:-1],h1[0],width=w,color='b')\n",
    "ax.bar(bins[:-1]+w,h2[0],width=w,color='g')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Tip (%)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Tip')\n",
    "ax.legend(['Non-Manhattan','Manhattah'],title='origin')\n",
    "plt.show()\n",
    "print 't-test results:', ttest_ind(v1,v2,equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two distributions look the same however the t-test results in a zero p-value to imply that the two groups are different at 95% level of condidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "#### Summary\n",
    "The initial dataset contained 1494926 transactions with 21 time-series, categorical and numerical variables. In order to build the final model, four phases were followed (1) data cleaning, (2) feature engineering (3) exploratory data analysis and (4) model creation\n",
    "\n",
    "The cleaning consisted in drop zero variance variables (Ehail_fee), replacing invalid with the most frequent values in each categorical variable whereas the median was used for continuous numerical variables. Invalid values could be missing values or values not allowed for specific variables as per the [dictionary of variables](http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf). In this part, variables were also converted in their appropriate format such datetime.\n",
    "\n",
    "The feature engineering part created 10 new variables derived from pickup and dropoff locations and timestamps, trip distance.\n",
    "\n",
    "During the exploration, each variable was carefully analyzed and compared to other variables and eventually the target variable, Percentage tip. All numerical variables were found to follow lognormal or power law distributions althouth there was found no linear relationship between numerical and the target variable. An interesting insight was uncovered in the distribution of the percentage tip. It was found that only 40% of the transactions paid tip. And 99.99% of these payments were done by credit cards. This inspired me to build the predictive model in two stages (1) classification model to find out weither a transaction will pay tip and (2) regression model to find the percentage of the tip only if the transaction was classified as a tipper. Another insight was that the most frequent percentage is 18% which corresponds to the usual restaurant gratuity rate.\n",
    "\n",
    "With lack of linear relationship between independent and depend variables, the predictive model was built on top of the random forest regression and gradient boosting classifier algorithms implemented in sklearn after routines to optimize best parameters. A usable script to make predictions as attached to this notebook and available in the same directory.\n",
    "\n",
    "\n",
    "**Note:** The code to make predictions is provided in the same directory as tip_predictor.py and the instructions are in the recommendation part of this section.\n",
    "\n",
    "Following, each part of the analysis is fully explained with accompanying python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Download the September 2015 dataset\n",
    "if os.path.exists('data_september_2015.csv'): # Check if the dataset is present on local disk and load it\n",
    "    data = pd.read_csv('data_september_2015.csv')\n",
    "else: # Download dataset if not available on disk\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv\"\n",
    "    data = pd.read_csv(url)\n",
    "    data.to_csv(url.split('/')[-1])\n",
    "\n",
    "# Print the size of the dataset\n",
    "print \"Number of rows:\", data.shape[0]\n",
    "print \"Number of columns: \", data.shape[1]\n",
    "\n",
    "# create backup dataset\n",
    "backup_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Data cleaning\n",
    "This part concerns work done to treat invalid data.\n",
    "\n",
    "- Ehail_fee was removed since 99% of the data are missing\n",
    "- Missing values in Trip_type were replace with the most common value that was 1\n",
    "- Invalid data were found in:\n",
    "    - RateCodeID: about 0.01% of the values were 99. These were replaced by the most common value 2\n",
    "    - Extra: 0.08% of transactions had negative Extra. These were replaced by 0 as the most frequent\n",
    "    - Total_amount, Fare_amount, improvement_surcharge, Tip_amount: 0.16% of values were negative. The cases were considered as being machine errors during the data entry. They were replaced by their absolute values. Furthermore, as the minimum Total_amount that is chargeable for any service is $2.5, every transaction falling below that amount was replaced by the median value of the Total_amount 11.76.\n",
    "\n",
    "The code is provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# define a function to clean a loaded dataset\n",
    "\n",
    "def clean_data(adata):\n",
    "    \"\"\"\n",
    "    This function cleans the input dataframe adata:\n",
    "    . drop Ehail_fee [99% transactions are NaNs]\n",
    "    . impute missing values in Trip_type\n",
    "    . replace invalid data by most frequent value for RateCodeID and Extra\n",
    "    . encode categorical to numeric\n",
    "    . rename pickup and dropff time variables (for later use)\n",
    "    \n",
    "    input:\n",
    "        adata: pandas.dataframe\n",
    "    output: \n",
    "        pandas.dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    ## make a copy of the input\n",
    "    data = adata.copy()\n",
    "    ## drop Ehail_fee: 99% of its values are NaNs\n",
    "    if 'Ehail_fee' in data.columns:\n",
    "        data.drop('Ehail_fee',axis=1,inplace=True)\n",
    "\n",
    "    ##  replace missing values in Trip_type with the most frequent value 1\n",
    "    data['Trip_type '] = data['Trip_type '].replace(np.NaN,1)\n",
    "    \n",
    "    ## replace all values that are not allowed as per the variable dictionary with the most frequent allowable value\n",
    "    # remove negative values from Total amound and Fare_amount\n",
    "    print \"Negative values found and replaced by their abs\"\n",
    "    print \"Total_amount\", 100*data[data.Total_amount<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    print \"Fare_amount\", 100*data[data.Fare_amount<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    print \"Improvement_surcharge\", 100*data[data.improvement_surcharge<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    print \"Tip_amount\", 100*data[data.Tip_amount<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    print \"Tolls_amount\", 100*data[data.Tolls_amount<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    print \"MTA_tax\", 100*data[data.MTA_tax<0].shape[0]/float(data.shape[0]),\"%\"\n",
    "    data.Total_amount = data.Total_amount.abs()\n",
    "    data.Fare_amount = data.Fare_amount.abs()\n",
    "    data.improvement_surcharge = data.improvement_surcharge.abs()\n",
    "    data.Tip_amount = data.Tip_amount.abs()\n",
    "    data.Tolls_amount = data.Tolls_amount.abs()\n",
    "    data.MTA_tax = data.MTA_tax.abs()\n",
    "    \n",
    "    # RateCodeID\n",
    "    indices_oi = data[~((data.RateCodeID>=1) & (data.RateCodeID<=6))].index\n",
    "    data.loc[indices_oi, 'RateCodeID'] = 2 # 2 = Cash payment was identified as the common method\n",
    "    print round(100*len(indices_oi)/float(data.shape[0]),2),\"% of values in RateCodeID were invalid.--> Replaced by the most frequent 2\"\n",
    "    \n",
    "    # Extra\n",
    "    indices_oi = data[~((data.Extra==0) | (data.Extra==0.5) | (data.Extra==1))].index\n",
    "    data.loc[indices_oi, 'Extra'] = 0 # 0 was identified as the most frequent value\n",
    "    print round(100*len(indices_oi)/float(data.shape[0]),2),\"% of values in Extra were invalid.--> Replaced by the most frequent 0\"\n",
    "    \n",
    "    # Total_amount: the minimum charge is 2.5, so I will replace every thing less than 2.5 by the median 11.76 (pre-obtained in analysis)\n",
    "    indices_oi = data[(data.Total_amount<2.5)].index\n",
    "    data.loc[indices_oi,'Total_amount'] = 11.76\n",
    "    print round(100*len(indices_oi)/float(data.shape[0]),2),\"% of values in total amount worth <$2.5.--> Replaced by the median 1.76\"\n",
    "    \n",
    "    # encode categorical to numeric (I avoid to use dummy to keep dataset small)\n",
    "    if data.Store_and_fwd_flag.dtype.name != 'int64':\n",
    "        data['Store_and_fwd_flag'] = (data.Store_and_fwd_flag=='Y')*1\n",
    "    \n",
    "    # rename time stamp variables and convert them to the right format\n",
    "    print \"renaming variables...\"\n",
    "    data.rename(columns={'lpep_pickup_datetime':'Pickup_dt','Lpep_dropoff_datetime':'Dropoff_dt'},inplace=True)\n",
    "    print \"converting timestamps variables to right format ...\"\n",
    "    data['Pickup_dt'] = data.Pickup_dt.apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "    data['Dropoff_dt'] = data.Dropoff_dt.apply(lambda x:dt.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    print \"Done cleaning\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Run code to clean the data\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Feature engineering\n",
    "In this step, I intuitively created new varibles derived from current variables. \n",
    "\n",
    "- Time variables: Week, Month_day(Day of month), Week_day (Day of week), Hour (hour of day), Shift_type (shift period of the day) and Trip_duration.The were created under the hypothesis that people may be willing to tip depending on the week days or time of the day. For instance, people are more friendly and less stressful to easily tip over the weekend. They were derived from pickup time\n",
    "- Trip directions: Direction_NS (is the cab moving Northt to South?) and Direction_EW (is the cab moving East to West). These are components of the two main directions, horizontal and vertical. The hypothesis is that the traffic may be different in different directions and it may affect the riders enthousiasm to tipping. They were derived from pickup and dropoff coordinates\n",
    "- Speed: this the ratio of Trip_distance to Trip_duration. At this level, all entries with speeds higher than 240 mph were dropped since this is the typical highest speed for cars commonly used as taxi [in addition to the fact that the speed limit in NYC is 50 mph](http://nytrafficticket.com/fastest-road-in-america-and-maximum-speed-limits-in-new-york/). An alternative filter threshold would be the highest posted speed limit in NYC but it might be sometimes violated.\n",
    "- With_tip: This is to identify transactions with tips or not. This variable was created after discovering that 60% of transactions have 0 tip.\n",
    "- As seen that using the mean of trips from Manhattan is different from the mean from other boroughs, this variable can be considered as well in the model building. A further and deep analysis, would be to create a variable of the origin and destination of each trip. This was tried but it was computationally excessive to my system. Here, coming from Manhattan or not, is the only variable to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Function to run the feature engineering\n",
    "def engineer_features(adata):\n",
    "    \"\"\"\n",
    "    This function create new variables based on present variables in the dataset adata. It creates:\n",
    "    . Week: int {1,2,3,4,5}, Week a transaction was done\n",
    "    . Week_day: int [0-6], day of the week a transaction was done\n",
    "    . Month_day: int [0-30], day of the month a transaction was done\n",
    "    . Hour: int [0-23], hour the day a transaction was done\n",
    "    . Shift type: int {1=(7am to 3pm), 2=(3pm to 11pm) and 3=(11pm to 7am)}, shift of the day  \n",
    "    . Speed_mph: float, speed of the trip\n",
    "    . Tip_percentage: float, target variable\n",
    "    . With_tip: int {0,1}, 1 = transaction with tip, 0 transction without tip\n",
    "    \n",
    "    input:\n",
    "        adata: pandas.dataframe\n",
    "    output: \n",
    "        pandas.dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # make copy of the original dataset\n",
    "    data = adata.copy()\n",
    "    \n",
    "    # derive time variables\n",
    "    print \"deriving time variables...\"\n",
    "    ref_week = dt.datetime(2015,9,1).isocalendar()[1] # first week of september in 2015\n",
    "    data['Week'] = data.Pickup_dt.apply(lambda x:x.isocalendar()[1])-ref_week+1\n",
    "    data['Week_day']  = data.Pickup_dt.apply(lambda x:x.isocalendar()[2])\n",
    "    data['Month_day'] = data.Pickup_dt.apply(lambda x:x.day)\n",
    "    data['Hour'] = data.Pickup_dt.apply(lambda x:x.hour)\n",
    "    #data.rename(columns={'Pickup_hour':'Hour'},inplace=True)\n",
    "\n",
    "    # create shift variable:  1=(7am to 3pm), 2=(3pm to 11pm) and 3=(11pm to 7am)\n",
    "    data['Shift_type'] = np.NAN\n",
    "    data.loc[data[(data.Hour>=7) & (data.Hour<15)].index,'Shift_type'] = 1\n",
    "    data.loc[data[(data.Hour>=15) & (data.Hour<23)].index,'Shift_type'] = 2\n",
    "    data.loc[data[data.Shift_type.isnull()].index,'Shift_type'] = 3\n",
    "    \n",
    "    # Trip duration \n",
    "    print \"deriving Trip_duration...\"\n",
    "    data['Trip_duration'] = ((data.Dropoff_dt-data.Pickup_dt).apply(lambda x:x.total_seconds()/60.))\n",
    "    \n",
    "    print \"deriving direction variables...\"\n",
    "    # create direction variable Direction_NS. \n",
    "    # This is 2 if taxi moving from north to south, 1 in the opposite direction and 0 otherwise\n",
    "    data['Direction_NS'] = (data.Pickup_latitude>data.Dropoff_latitude)*1+1\n",
    "    indices = data[(data.Pickup_latitude == data.Dropoff_latitude) & (data.Pickup_latitude!=0)].index\n",
    "    data.loc[indices,'Direction_NS'] = 0\n",
    "\n",
    "    # create direction variable Direction_EW. \n",
    "    # This is 2 if taxi moving from east to west, 1 in the opposite direction and 0 otherwise\n",
    "    data['Direction_EW'] = (data.Pickup_longitude>data.Dropoff_longitude)*1+1\n",
    "    indices = data[(data.Pickup_longitude == data.Dropoff_longitude) & (data.Pickup_longitude!=0)].index\n",
    "    data.loc[indices,'Direction_EW'] = 0\n",
    "    \n",
    "    # create variable for Speed\n",
    "    print \"deriving Speed. Make sure to check for possible NaNs and Inf vals...\"\n",
    "    data['Speed_mph'] = data.Trip_distance/(data.Trip_duration/60)\n",
    "    # replace all NaNs values and values >240mph by a values sampled from a random distribution of \n",
    "    # mean 12.9 and  standard deviation 6.8mph. These values were extracted from the distribution\n",
    "    indices_oi = data[(data.Speed_mph.isnull()) | (data.Speed_mph>240)].index\n",
    "    data.loc[indices_oi,'Speed_mph'] = np.abs(np.random.normal(loc=12.9,scale=6.8,size=len(indices_oi)))\n",
    "    print \"Feature engineering done! :-)\"\n",
    "    \n",
    "    # Create a new variable to check if a trip originated in Upper Manhattan\n",
    "    print \"checking where the trip originated...\"\n",
    "    data['U_manhattan'] = data[['Pickup_latitude','Pickup_longitude']].apply(lambda r:is_within_bbox((r[0],r[1])),axis=1)\n",
    "    \n",
    "    # create tip percentage variable\n",
    "    data['Tip_percentage'] = 100*data.Tip_amount/data.Total_amount\n",
    "    \n",
    "    # create with_tip variable\n",
    "    data['With_tip'] = (data.Tip_percentage>0)*1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# collected bounding box points\n",
    "umanhattan = [(40.796937, -73.949503),(40.787945, -73.955822),(40.782772, -73.943575),\n",
    "              (40.794715, -73.929801),(40.811261, -73.934153),(40.835371, -73.934515),\n",
    "              (40.868910, -73.911145),(40.872719, -73.910765),(40.878252, -73.926350),\n",
    "              (40.850557, -73.947262),(40.836225, -73.949899),(40.806050, -73.971255)]\n",
    "\n",
    "poi = Polygon(umanhattan)\n",
    "# create a function to check if a location is located inside Upper Manhattan\n",
    "def is_within_bbox(loc,poi=poi):\n",
    "    \"\"\"\n",
    "    This function checks if a location loc with lat and lon is located within the polygon of interest\n",
    "    input:\n",
    "    loc: tuple, (latitude, longitude)\n",
    "    poi: shapely.geometry.Polygon, polygon of interest\n",
    "    \"\"\"\n",
    "    return 1*(Point(loc).within(poi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# run the code to create new features on the dataset\n",
    "print \"size before feature engineering:\", data.shape\n",
    "data = engineer_features(data)\n",
    "print \"size after feature engineering:\", data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Uncomment to check for data validity. \n",
    "# data.describe() .T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Exploratory Data Analysis\n",
    "This was the key phase of my analysis. A look at the distribution of the target variable, \"Tip_percentage\" showed that 60% of all transactions did not give tip (see Figure below, left). A second tip at 18% corresponds to the usual NYC customary gratuity rate which fluctuates between 18% and 25% (see Figure below,right). Based on this information, the model can be built in two steps\n",
    "\n",
    "1. Create classification model to predict weither tip will be given or not. Here a new variable \"With_tip\" of 1 (if there is tip) and 0 (otherwise) was created.\n",
    "2. Create regression model for transaction with non-zero tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## code to compare the two Tip_percentage identified groups\n",
    "# split data in the two groups\n",
    "data1 = data[data.Tip_percentage>0]\n",
    "data2 = data[data.Tip_percentage==0]\n",
    "\n",
    "# generate histograms to compare\n",
    "fig,ax=plt.subplots(1,2,figsize=(14,4))\n",
    "data.Tip_percentage.hist(bins = 20,normed=True,ax=ax[0])\n",
    "ax[0].set_xlabel('Tip (%)')\n",
    "ax[0].set_title('Distribution of Tip (%) - All transactions')\n",
    "\n",
    "data1.Tip_percentage.hist(bins = 20,normed=True,ax=ax[1])\n",
    "ax[1].set_xlabel('Tip (%)')\n",
    "ax[1].set_title('Distribution of Tip (%) - Transaction with tips')\n",
    "ax[1].set_ylabel('Group normed count')\n",
    "plt.savefig('Question4_target_varc.jpeg',format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, each variable distribution and its relationship with the Tip percentage were explored. Few functions were implemented to quickly explore those variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Functions for exploratory data analysis\n",
    "def visualize_continuous(df,label,method={'type':'histogram','bins':20},outlier='on'):\n",
    "    \"\"\"\n",
    "    function to quickly visualize continous variables\n",
    "    df: pandas.dataFrame \n",
    "    label: str, name of the variable to be plotted. It should be present in df.columns\n",
    "    method: dict, contains info of the type of plot to generate. It can be histogram or boxplot [-Not yet developped]\n",
    "    outlier: {'on','off'}, Set it to off if you need to cut off outliers. Outliers are all those points\n",
    "    located at 3 standard deviations further from the mean\n",
    "    \"\"\"\n",
    "    # create vector of the variable of interest\n",
    "    v = df[label]\n",
    "    # define mean and standard deviation\n",
    "    m = v.mean()\n",
    "    s = v.std()\n",
    "    # prep the figure\n",
    "    fig,ax = plt.subplots(1,2,figsize=(14,4))\n",
    "    ax[0].set_title('Distribution of '+label)\n",
    "    ax[1].set_title('Tip % by '+label)\n",
    "    if outlier=='off': # remove outliers accordingly and update titles\n",
    "        v = v[(v-m)<=3*s]\n",
    "        ax[0].set_title('Distribution of '+label+'(no outliers)')\n",
    "        ax[1].set_title('Tip % by '+label+'(no outliers)')\n",
    "    if method['type'] == 'histogram': # plot the histogram\n",
    "        v.hist(bins = method['bins'],ax=ax[0])\n",
    "    if method['type'] == 'boxplot': # plot the box plot\n",
    "        df.loc[v.index].boxplot(label,ax=ax[0])\n",
    "    ax[1].plot(v,df.loc[v.index].Tip_percentage,'.',alpha=0.4)\n",
    "    ax[0].set_xlabel(label)\n",
    "    ax[1].set_xlabel(label)\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[1].set_ylabel('Tip (%)')\n",
    "\n",
    "def visualize_categories(df,catName,chart_type='histogram',ylimit=[None,None]):\n",
    "    \"\"\"\n",
    "    This functions helps to quickly visualize categorical variables. \n",
    "    This functions calls other functions generate_boxplot and generate_histogram\n",
    "    df: pandas.Dataframe\n",
    "    catName: str, variable name, it must be present in df\n",
    "    chart_type: {histogram,boxplot}, choose which type of chart to plot\n",
    "    ylim: tuple, list. Valid if chart_type is histogram\n",
    "    \"\"\"\n",
    "    print catName\n",
    "    cats = sorted(pd.unique(df[catName]))\n",
    "    if chart_type == 'boxplot': #generate boxplot\n",
    "        generate_boxplot(df,catName,ylimit)\n",
    "    elif chart_type == 'histogram': # generate histogram\n",
    "        generate_histogram(df,catName)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #=> calculate test statistics\n",
    "    groups = df[[catName,'Tip_percentage']].groupby(catName).groups #create groups\n",
    "    tips = df.Tip_percentage\n",
    "    if len(cats)<=2: # if there are only two groups use t-test\n",
    "        print ttest_ind(tips[groups[cats[0]]],tips[groups[cats[1]]])\n",
    "    else: # otherwise, use one_way anova test\n",
    "        # prepare the command to be evaluated\n",
    "        cmd = \"f_oneway(\"\n",
    "        for cat in cats:\n",
    "            cmd+=\"tips[groups[\"+str(cat)+\"]],\"\n",
    "        cmd=cmd[:-1]+\")\"\n",
    "        print \"one way anova test:\", eval(cmd) #evaluate the command and print\n",
    "    print \"Frequency of categories (%):\\n\",df[catName].value_counts(normalize=True)*100\n",
    "    \n",
    "def test_classification(df,label,yl=[0,50]):\n",
    "    \"\"\"\n",
    "    This function test if the means of the two groups with_tip and without_tip are different at 95% of confidence level.\n",
    "    It will also generate a box plot of the variable by tipping groups\n",
    "    label: str, label to test\n",
    "    yl: tuple or list (default = [0,50]), y limits on the ylabel of the boxplot\n",
    "    df: pandas.DataFrame (default = data)\n",
    "    \n",
    "    Example: run <visualize_continuous(data,'Fare_amount',outlier='on')>\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pd.unique(df[label]))==2: #check if the variable is categorical with only two  categores and run chisquare test\n",
    "        vals=pd.unique(df[label])\n",
    "        gp1 = df[df.With_tip==0][label].value_counts().sort_index()\n",
    "        gp2 = df[df.With_tip==1][label].value_counts().sort_index()\n",
    "        print \"t-test if\", label, \"can be used to distinguish transaction with tip and without tip\"\n",
    "        print chisquare(gp1,gp2)\n",
    "    elif len(pd.unique(df[label]))>=10: #other wise  run the t-test\n",
    "        df.boxplot(label,by='With_tip')\n",
    "        plt.ylim(yl)\n",
    "        plt.show()\n",
    "        print \"t-test if\", label, \"can be used to distinguish transaction with tip and without tip\"\n",
    "        print \"results:\",ttest_ind(df[df.With_tip==0][label].values,df[df.With_tip==1][label].values,False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def generate_boxplot(df,catName,ylimit):\n",
    "    \"\"\"\n",
    "    generate boxplot of tip percentage by variable \"catName\" with ylim set to ylimit\n",
    "    df: pandas.Dataframe\n",
    "    catName: str\n",
    "    ylimit: tuple, list\n",
    "    \"\"\"\n",
    "    df.boxplot('Tip_percentage',by=catName)\n",
    "    #plt.title('Tip % by '+catName)\n",
    "    plt.title('')\n",
    "    plt.ylabel('Tip (%)')\n",
    "    if ylimit != [None,None]:\n",
    "        plt.ylim(ylimit)\n",
    "    plt.show()\n",
    "\n",
    "def generate_histogram(df,catName):\n",
    "    \"\"\"\n",
    "    generate histogram of tip percentage by variable \"catName\" with ylim set to ylimit\n",
    "    df: pandas.Dataframe\n",
    "    catName: str\n",
    "    ylimit: tuple, list\n",
    "    \"\"\"\n",
    "    cats = sorted(pd.unique(df[catName]))\n",
    "    colors = plt.cm.jet(np.linspace(0,1,len(cats)))\n",
    "    hx = np.array(map(lambda x:round(x,1),np.histogram(df.Tip_percentage,bins=20)[1]))\n",
    "    fig,ax = plt.subplots(1,1,figsize = (15,4))\n",
    "    for i,cat in enumerate(cats):\n",
    "        vals = df[df[catName] == cat].Tip_percentage\n",
    "        h = np.histogram(vals,bins=hx)\n",
    "        w = 0.9*(hx[1]-hx[0])/float(len(cats))\n",
    "        plt.bar(hx[:-1]+w*i,h[0],color=colors[i],width=w)\n",
    "    plt.legend(cats)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Distribution of Tip by '+catName)\n",
    "    plt.xlabel('Tip (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with continuous variables, two main insights were discovered: A lognormal-like or power law distribution of the Fare amount and a non-linear function of the tip percentage as function of the total amount. The tip percentage decreases as the fare amount increases but converges around 20%. The density of scattered points implies that there is a high frequency of smaller tipps at low Fare_amount. Can we say that people restrain themselves to tipping more money as the cost of the ride becomes more and more expensive? Or since the fare grows with the length of the trip and trip duration, can we say that riders get bored and don't appreciate the service they are getting? Many questsions can be explored at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Example of exploration of the Fare_amount using the implented code:\n",
    "visualize_continuous(data1,'Fare_amount',outlier='on')\n",
    "test_classification(data,'Fare_amount',[0,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative t-test value and null p-value imply that the means of Total_amount are significantly different in the group of transactions with tips compared to the group with no tip. Therefore, this variable would used to train the classification model.\n",
    "\n",
    "Using the same function, a plot of the tip percentage as function of trip duration showed a cluster of points at duration time greater than 1350 min (22 hours). \n",
    "\n",
    "<img src=\"Q4_trip_duration.png\">\n",
    "\n",
    "These points look like outliers since it doesn't make sense to have a trip of 22 hours within NYC. Probably, tourists can! \n",
    "\n",
    "The following code was used to analyze the cluser with trip duration greater than 1350 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Code to generate the heat map to uncover hidden information in the cluster\n",
    "# We will first source NYC boroughs shape files, \n",
    "# then create polygons and check to which polygon does each of the point of the cluster begongs\n",
    "\n",
    "## download geojson of NYC boroughs\n",
    "nyc_boros = json.loads(requests.get(\"https://raw.githubusercontent.com/dwillis/nyc-maps/master/boroughs.geojson\").content)\n",
    "\n",
    "# parse boros into Multipolygons\n",
    "boros = {}\n",
    "for f in nyc_boros['features']:\n",
    "    name = f['properties']['BoroName']\n",
    "    code = f['properties']['BoroCode']\n",
    "    polygons = []\n",
    "    for p in f['geometry']['coordinates']:\n",
    "        polygons.append(Polygon(p[0]))\n",
    "    boros[code] = {'name':name,'polygon':MultiPolygon(polygons=polygons)}\n",
    "    \n",
    "# creae function to assign each coordinates point to its borough\n",
    "def find_borough(lat,lon):\n",
    "    \"\"\"\n",
    "    return the borough of a location given its latitude and longitude\n",
    "    lat: float, latitude\n",
    "    lon: float, longitude\n",
    "    \"\"\"\n",
    "    boro = 0 # initialize borough as 0\n",
    "    for k,v in boros.iteritems(): # update boro to the right key corresponding to the parent polygon\n",
    "        if v['polygon'].contains(Point(lon,lat)):\n",
    "            boro = k\n",
    "            break # break the loop once the borough is found\n",
    "    return [boro]\n",
    "\n",
    "## Analyse the cluster now\n",
    "# create data frame of boroughs\n",
    "df = data1[data1.Trip_duration>=1350]\n",
    "orig_dest = []\n",
    "for v in df[['Pickup_latitude','Pickup_longitude','Dropoff_latitude','Dropoff_longitude']].values:\n",
    "    orig_dest.append((find_borough(v[0],v[1])[0],find_borough(v[2],v[3])[0]))\n",
    "df2 = pd.DataFrame(orig_dest)\n",
    "\n",
    "## creae pivot table for the heat map plot\n",
    "df2['val']=1 # dummy variable\n",
    "mat_cluster1 = df2.pivot_table(index=0,columns=1,values='val',aggfunc='count')\n",
    "\n",
    "## generate the map\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
    "im = ax[0].imshow(mat_cluster1)\n",
    "ax[0].set_ylabel('From')\n",
    "ax[0].set_xlabel('To')\n",
    "ax[0].set_xticklabels(['','Other','Manhattan','Bronx','Brooklyn','Queens'],rotation='vertical')\n",
    "ax[0].set_yticklabels(['','Other','Manhattan','Bronx','Brooklyn','Queens'])\n",
    "ax[0].set_title('Cluster of rides with duration >1350 min')\n",
    "fig.colorbar(im,ax=ax[0])\n",
    "h = df.Hour.value_counts(normalize=True)\n",
    "plt.bar(h.index,h.values,width = .4,color='b')\n",
    "h = data1.Hour.value_counts(normalize=True)\n",
    "ax[1].bar(h.index+.4,h.values,width = .4,color='g')\n",
    "ax[1].set_title('Hourly traffic: All rides vs cluster rides')\n",
    "ax[1].legend(['cluster','all'],loc='best')\n",
    "ax[1].set_xlabel('Hour')\n",
    "ax[1].set_xticks(np.arange(25)+.4,range(25))\n",
    "ax[1].set_ylabel('Normalized Count')\n",
    "plt.savefig('duration_cluster.jpeg',format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map color represents the number of trips between two given boroughs. We can see that the majority of the trips  are intra-boroughs. There is a great number of trips from Brooklyn to Manhattan whereas there is no Staten Island trip that takes more than 1350 minutes. Are there specific hours for these events? Unfortunately, the distribution on the rigtht shows that the cluster behaves the same as the rest of the traffic.\n",
    "\n",
    "\n",
    "Finally, correlation heatmap was used to find which independent variables are correlated to each other. The following code provides the construction of the correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "continuous_variables=['Total_amount','Fare_amount','Trip_distance','Trip_duration','Tolls_amount','Speed_mph','Tip_percentage']\n",
    "cor_mat = data1[continuous_variables].corr()\n",
    "#fig,ax = plt.subplots(1,1,figsize = [6,6])\n",
    "plt.imshow(cor_mat)\n",
    "plt.xticks(range(len(continuous_variables)),continuous_variables,rotation='vertical')\n",
    "plt.yticks(range(len(continuous_variables)),continuous_variables)\n",
    "plt.colorbar()\n",
    "plt.title('Correlation between continuous variables')\n",
    "plt.show()\n",
    "#print cor_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A further analysis of all continuous variables revealed similar lognormal and non-linearlity behaviors. Since there is no linear relationship between the the Tip percentage and variables, random forest algorithm will be considered to build the regression part of the model***\n",
    "\n",
    "As far as categorical variables concerned, the function visualize_categories was used to explore each variable as it was done for continuous numerical variables (See demostration below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# exploration of the U_manhattan (trip originating from Upper Manhattan) variable \n",
    "visualize_categories(data1,'U_manhattan','boxplot',[13,20])\n",
    "test_classification(data,'U_manhattan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot compares the means and range of the Tip_percentage between trips originating from Manhattan and the rest of the trips. The t-test reported says that these groups have different means. Furthermore, a chi-square test shows that this variable can be used to significantly distinguish transactions with tips from those without tips. \n",
    "\n",
    "Another interesting figure is that of the Payment_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# visualization of the Payment_type\n",
    "visualize_categories(data1,'Payment_type','histogram',[13,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution shows that 99.99% transactions with tips were paid by Credit Card (method 1). This variable is not a good candidate for the regression model because of this unbalanced frequenced but it is eventually an important feature to use in the classification model. An intuitive rule would be that if a rider is not paying with a credit card, there will be no tip.\n",
    "\n",
    "Similar analysis were carried on every variable in order to find the most important variables with enough variance for either the regression model and/or classification model. This visual exploration analysis and statistical tests section concluded by selecting Total_amount, Fare_amount, Trip_distance, Tolls_amount, Trip_duration, Speed_mph, U_manhattan, Direction_NS and Direction_EW as initial important features to train and optimized the regression model. Payment_type, Passenger_count, Extra, Week_day, Hour, Direction_NS, Direction_EW, U_manhattan and Shift_type were selected as initial variables to train the classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Building the Model\n",
    "\n",
    "As explained in the previous section, this model will be a combination of rules from two models (1) The classification model to classify a transaction into a tipper (=1) or not (=0)and (2) regression model to estimate the percentage of the tip given that the results from the classification model was 1\n",
    "\n",
    "First of all, functions for cross-validation and parameter optimization were defined such that they can be used on either classification or regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import scikit learn libraries\n",
    "from sklearn import cross_validation, metrics   #model optimization and valuation tools\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# define a function that help to train models and perform cv\n",
    "def modelfit(alg,dtrain,predictors,target,scoring_method,performCV=True,printFeatureImportance=True,cv_folds=5):\n",
    "    \"\"\"\n",
    "    This functions train the model given as 'alg' by performing cross-validation. It works on both regression and classification\n",
    "    alg: sklearn model\n",
    "    dtrain: pandas.DataFrame, training set\n",
    "    predictors: list, labels to be used in the model training process. They should be in the column names of dtrain\n",
    "    target: str, target variable\n",
    "    scoring_method: str, method to be used by the cross-validation to valuate the model\n",
    "    performCV: bool, perform Cv or not\n",
    "    printFeatureImportance: bool, plot histogram of features importance or not\n",
    "    cv_folds: int, degree of cross-validation\n",
    "    \"\"\"\n",
    "    # train the algorithm on data\n",
    "    alg.fit(dtrain[predictors],dtrain[target])\n",
    "    #predict on train set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    if scoring_method == 'roc_auc':\n",
    "        dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg,dtrain[predictors],dtrain[target],cv=cv_folds,scoring=scoring_method)\n",
    "        #print model report\n",
    "        print \"\\nModel report:\"\n",
    "        if scoring_method == 'roc_auc':\n",
    "            print \"Accuracy:\",metrics.accuracy_score(dtrain[target].values,dtrain_predictions)\n",
    "            print \"AUC Score (Train):\",metrics.roc_auc_score(dtrain[target], dtrain_predprob)\n",
    "        if (scoring_method == 'mean_squared_error'):\n",
    "            print \"Accuracy:\",metrics.mean_squared_error(dtrain[target].values,dtrain_predictions)\n",
    "    if performCV:\n",
    "        print \"CV Score - Mean : %.7g | Std : %.7g | Min : %.7g | Max : %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n",
    "    #print feature importance\n",
    "    if printFeatureImportance:\n",
    "        if dir(alg)[0] == '_Booster': #runs only if alg is xgboost\n",
    "            feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "        else:\n",
    "            feat_imp = pd.Series(alg.feature_importances_,predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar',title='Feature Importances')\n",
    "        plt.ylabel('Feature Importe Score')\n",
    "        plt.show()\n",
    "\n",
    "# optimize n_estimator through grid search\n",
    "def optimize_num_trees(alg,param_test,scoring_method,train,predictors,target):\n",
    "    \"\"\"\n",
    "    This functions is used to tune paremeters of a predictive algorithm\n",
    "    alg: sklearn model,\n",
    "    param_test: dict, parameters to be tuned\n",
    "    scoring_method: str, method to be used by the cross-validation to valuate the model\n",
    "    train: pandas.DataFrame, training data\n",
    "    predictors: list, labels to be used in the model training process. They should be in the column names of dtrain\n",
    "    target: str, target variable\n",
    "    \"\"\"\n",
    "    gsearch = GridSearchCV(estimator=alg, param_grid = param_test, scoring=scoring_method,n_jobs=2,iid=False,cv=5)\n",
    "    gsearch.fit(train[predictors],train[target])\n",
    "    return gsearch\n",
    "\n",
    "# plot optimization results\n",
    "def plot_opt_results(alg):\n",
    "    cv_results = []\n",
    "    for i in range(len(param_test['n_estimators'])):\n",
    "        cv_results.append((alg.grid_scores_[i][1],alg.grid_scores_[i][0]['n_estimators']))\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    plt.plot(cv_results[1],cv_results[0])\n",
    "    plt.xlabel('# trees')\n",
    "    plt.ylabel('score')\n",
    "    plt.title('optimization report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1. Classification Model \n",
    "After spending a time on feature exploration, engineering and discovering that the Payment_type was a strong variable (99.99% of all transactions with tips were paid with credit cards) to differentiate transactions with tip from those without tip, a model based on the logistic regression classifier algorithm was optimized and gave an accuracy score of 0.94. However, this was outperformed by using the GradientBoostingClassifier (from scikit learn) which gave a score of 0.96. Starting with the GradientBoostinClassier model (default paremeters), the number of trees was optimized through a grid search (see function 'optimize_num_trees').\n",
    "\n",
    "*-- Key points --*:\n",
    "\n",
    "- Sample size for training and optimization was chosen as 100000. This is surely a small sample size compared to the available data but the optimization was stable and good enough with 5 folds cross-validation\n",
    "- Only the number of trees were optimized as they are the controlling key of boosting model accuracy. Other parameters were not optimized since the improvement yield was too small compared to the computation time and cost\n",
    "- ROC-AUC (Area under the curve of receiver operating characteristic) was used as a model validation metric\n",
    "\n",
    "*-- Results --*:\n",
    "\n",
    "- optimized number of trees: 130\n",
    "- optimized variables: ['Payment_type','Total_amount','Trip_duration','Speed_mph','MTA_tax','Extra','Hour','Direction_NS', 'Direction_EW','U_manhattan']\n",
    "- roc-auc on a different test sample: 0.9636\n",
    "\n",
    "The following code shows the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## OPTIMIZATION & TRAINING OF THE CLASSIFIER\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print \"Optimizing the classifier...\"\n",
    "\n",
    "train = data.copy() # make a copy of the training set\n",
    "# since the dataset is too big for my system, select a small sample size to carry on training and 5 folds cross validation\n",
    "train = train.loc[np.random.choice(train.index,size=100000,replace=False)]\n",
    "target = 'With_tip' # set target variable - it will be used later in optimization\n",
    "\n",
    "tic = dt.datetime.now() # initiate the timing\n",
    "# for predictors start with candidates identified during the EDA\n",
    "predictors = ['Payment_type','Total_amount','Trip_duration','Speed_mph','MTA_tax',\n",
    "              'Extra','Hour','Direction_NS', 'Direction_EW','U_manhattan']\n",
    "\n",
    "# optimize n_estimator through grid search\n",
    "param_test = {'n_estimators':range(30,151,20)} # define range over which number of trees is to be optimized\n",
    "\n",
    "\n",
    "# initiate classification model\n",
    "model_cls = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, # use default\n",
    "    min_samples_split=2,# use default\n",
    "    max_depth=5,\n",
    "    max_features='auto',\n",
    "    subsample=0.8, # try <1 to decrease variance and increase bias\n",
    "    random_state = 10)\n",
    "\n",
    "# get results of the search grid\n",
    "gs_cls = optimize_num_trees(model_cls,param_test,'roc_auc',train,predictors,target)\n",
    "print gs_cls.grid_scores_, gs_cls.best_params_, gs_cls.best_score_\n",
    "\n",
    "# cross validate the best model with optimized number of estimators\n",
    "modelfit(gs_cls.best_estimator_,train,predictors,target,'roc_auc')\n",
    "\n",
    "# save the best estimator on disk as pickle for a later use\n",
    "with open('my_classifier.pkl','wb') as fid:\n",
    "    pickle.dump(gs_cls.best_estimator_,fid)\n",
    "    fid.close()\n",
    "    \n",
    "print \"Processing time:\", dt.datetime.now()-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the optimum number of trees in 130 and that the important variables for this specific number of tree are as shown on the barchart. \n",
    "\n",
    "Let's test it on a different sample with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# testing on a different set\n",
    "indices = data.index[~data.index.isin(train.index)]\n",
    "test = data.loc[np.random.choice(indices,size=100000,replace=False)]\n",
    "\n",
    "ypred = gs_cls.best_estimator_.predict(test[predictors])\n",
    "\n",
    "print \"ROC AUC:\", metrics.roc_auc_score(ypred,test.With_tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Regression Model \n",
    "Following a similar pipeline of optimization as in the classification model, a model was built on top of the random forest algorithm. \n",
    "\n",
    "*-- Key points --*:\n",
    "- Sample size for training and optimization was chosen as 100000 with 5 folds cross-validation\n",
    "- Only the number of trees were optimized as they are the controlling key of boosting model accuracy. Other parameters were not optimized since the improvement yield was too small compared to the computation time and cost\n",
    "- The mean square error was used as a valuation metric\n",
    "\n",
    "*-- Results --*:\n",
    "\n",
    "- optimized number of trees: 150\n",
    "- optimized variables: Total_amount, Trip_duration, Speed_mph\n",
    "- mean square error on a different test sample: 14.3648\n",
    "\n",
    "The following code shows the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train = data1.copy()\n",
    "train = train.loc[np.random.choice(train.index,size=100000,replace=False)]\n",
    "indices = data1.index[~data1.index.isin(train.index)]\n",
    "test = data1.loc[np.random.choice(indices,size=100000,replace=False)]\n",
    "\n",
    "train['ID'] = train.index\n",
    "IDCol = 'ID'\n",
    "target = 'Tip_percentage'\n",
    "\n",
    "predictors = ['VendorID', 'Passenger_count', 'Trip_distance', 'Total_amount', \n",
    "              'Extra', 'MTA_tax', 'Tolls_amount', 'Payment_type', \n",
    "              'Hour', 'U_manhattan', 'Week', 'Week_day', 'Month_day', 'Shift_type', \n",
    "              'Direction_NS', 'Direction_EW', 'Trip_duration', 'Speed_mph']\n",
    "predictors = ['Trip_distance','Tolls_amount', 'Direction_NS', 'Direction_EW', 'Trip_duration', 'Speed_mph']\n",
    "predictors = ['Total_amount', 'Trip_duration', 'Speed_mph']\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "tic = dt.datetime.now()\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# optimize n_estimator through grid search\n",
    "param_test = {'n_estimators':range(50,200,25)} # define range over which number of trees is to be optimized\n",
    "# initiate classification model\n",
    "#rfr = RandomForestRegressor(min_samples_split=2,max_depth=5,max_features='auto',random_state = 10)\n",
    "rfr = RandomForestRegressor()#n_estimators=100)\n",
    "# get results of the search grid\n",
    "gs_rfr = optimize_num_trees(rfr,param_test,'mean_squared_error',train,predictors,target)\n",
    "\n",
    "# print optimization results\n",
    "print gs_rfr.grid_scores_, gs_rfr.best_params_, gs_rfr.best_score_\n",
    "\n",
    "# plot optimization results\n",
    "#plot_opt_results(gs_rfr)\n",
    "\n",
    "# cross validate the best model with optimized number of estimators\n",
    "modelfit(gs_rfr.best_estimator_,train,predictors,target,'mean_squared_error')\n",
    "\n",
    "# save the best estimator on disk as pickle for a later use\n",
    "with open('my_rfr_reg2.pkl','wb') as fid:\n",
    "    pickle.dump(gs_rfr.best_estimator_,fid)\n",
    "    fid.close()\n",
    "\n",
    "ypred = gs_rfr.best_estimator_.predict(test[predictors])\n",
    "\n",
    "print 'RFR test mse:',metrics.mean_squared_error(ypred,test.Tip_percentage)\n",
    "print 'RFR r2:', metrics.r2_score(ypred,test.Tip_percentage)\n",
    "print dt.datetime.now()-tic\n",
    "plot_opt_results(gs_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the optimum number of trees in 150 and that the important variables for this specific number of tree are as shown on the barchart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Final Model \n",
    "\n",
    "This is a combination of the classification model and regression model in order to get the final predictions. The model was run on the entire dataset to predict expected tip percentages. It resulted in a mean squared error of 0.8793.\n",
    "\n",
    "The process is as follow:\n",
    "1. get transaction to  predict\n",
    "2. classify the transaction into zero and non-zero tip\n",
    "3. if the transaction is classified as non-zero, predict the tip percentage otherwise return 0\n",
    "\n",
    "Next, I define a function that is to be used to make final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def predict_tip(transaction):\n",
    "    \"\"\"\n",
    "    This function predicts the percentage tip expected on 1 transaction\n",
    "    transaction: pandas.dataframe, this should have been cleaned first and feature engineered\n",
    "    \"\"\"\n",
    "    # define predictors labels as per optimization results\n",
    "    cls_predictors = ['Payment_type','Total_amount','Trip_duration','Speed_mph','MTA_tax',\n",
    "                      'Extra','Hour','Direction_NS', 'Direction_EW','U_manhattan']\n",
    "    reg_predictors = ['Total_amount', 'Trip_duration', 'Speed_mph']\n",
    "    \n",
    "    # classify transactions\n",
    "    clas = gs_cls.best_estimator_.predict(transaction[cls_predictors])\n",
    "    \n",
    "    # predict tips for those transactions classified as 1\n",
    "    return clas*gs_rfr.best_estimator_.predict(transaction[reg_predictors])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a sample of 100000 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "test = data.loc[np.random.choice(data.index,size = 100000,replace=False)]\n",
    "ypred = predict_tip(test)\n",
    "print \"final mean_squared_error:\", metrics.mean_squared_error(ypred,test.Tip_percentage)\n",
    "print \"final r2_score:\", metrics.r2_score(ypred,test.Tip_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty good for a black box model.\n",
    "\n",
    "Finally, I plot the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df = test.copy() # make a copy of data\n",
    "df['predictions'] = ypred # add predictions column\n",
    "df['residuals'] = df.Tip_percentage - df.predictions # calculate residuals\n",
    "\n",
    "df.residuals.hist(bins = 20) # plot histogram of residuals\n",
    "plt.yscale('log')\n",
    "plt.xlabel('predicted - real')\n",
    "plt.ylabel('count')\n",
    "plt.title('Residual plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual is pretty much symmetrically distributed. This indicate that the model is equally biased. The best model would be the one with mean at 0 and 0 variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "\n",
    "- As a future work, I would find a transformation function that can linearlize indipendent values. I would also optimize different algorithms such as extreme gradient boosting and make a bag of multiple models as a final model. This was actually tried but failed because of the computational power. \n",
    "\n",
    "- The following section is the instruction on how to use the predictor script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def read_me():\n",
    "    \"\"\"\n",
    "    This is a function to print a read me instruction\n",
    "    \"\"\"\n",
    "    print (\"=========Introduction=========\\n\\nUse this code to predict the percentage tip expected after a trip in NYC green taxi. \\nThe code is a predictive model that was built and trained on top of the Gradient Boosting Classifer and the Random Forest Gradient both provided in scikit-learn\\n\\nThe input: \\npandas.dataframe with columns:This should be in the same format as downloaded from the website\\n\\nThe data frame go through the following pipeline:\\n\\t1. Cleaning\\n\\t2. Creation of derived variables\\n\\t3. Making predictions\\n\\nThe output:\\n\\tpandas.Series, two files are saved on disk,  submission.csv and cleaned_data.csv respectively.\\n\\nTo make predictions, run 'tip_predictor.make_predictions(data)', where data is any 2015 raw dataframe fresh from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\\nRun tip_predictor.read_me() for further instructions\\n\")\n",
    "read_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset is very rich of information and can be used to learn about other aspects of traffice in NYC. For instance, here I give a very small preview of an upcoming analysis of the speed\n",
    "\n",
    "    - Let's build a derived variable representing the average speed over the course of a trip\n",
    "    - I  perform a test to determine if the average speeds are materially the same in all weeks of September. I will use pairwise t-student test. The null hypothesis for t-test is that the mean is the same in two tested samples. We will see that the speed is not really different in all weeks. From the hypothesis test, we see that we don't have enough evidence that speeds are different in Week 2 and Week 3 are significantly different hence we fail to reject the null hypothesis at 95% level of confidence. The rest of the weeks have smaller p-values, so **we can reject the null hypothesis and say that they are significantly different. In general, the speed can be dependent of the week of the month. It would be interesting to look at data of August and October as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print \"mean speed by week:\\n\", data[['Speed_mph','Week']].groupby('Week').mean()\n",
    "# generate boxplot\n",
    "data.boxplot('Speed_mph','Week')\n",
    "plt.ylim([0,20]) # cut off outliers\n",
    "plt.ylabel('Speed (mph)')\n",
    "plt.show()\n",
    "\n",
    "# calculate t-test\n",
    "weeks = pd.unique(data.Week)\n",
    "pvals = []\n",
    "for i in range(len(weeks)): # for each pair, run t-test\n",
    "    for j in range(len(weeks)):\n",
    "        pvals.append((weeks[i], weeks[j],ttest_ind(data[data.Week==weeks[i]].Speed_mph,data[data.Week==weeks[j]].Speed_mph,False)[1]))\n",
    "    \n",
    "pvalues = pd.DataFrame(pvals,columns=['w1','w2','pval'])\n",
    "print \"p-values:\\n\",pvalues.pivot_table(index='w1',columns='w2',values='pval').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        - Another interesting question is how the speed changes over the course of the day. In this case I use  one-way anova test on multiple samples. We find that the speed is different in different hours with a zero pvalue of the anova test. The boxplot reveals that traffic is faster early morning and gets really slow in the evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# calculate anova\n",
    "hours = range(24)\n",
    "cmd = \"f_oneway(\"\n",
    "for h in hours:\n",
    "    cmd+=\"data[data.Hour==\"+str(h)+\"].Speed_mph,\"\n",
    "cmd=cmd[:-1]+\")\"\n",
    "print \"one way anova test:\", eval(cmd) #evaluate the command and print\n",
    "\n",
    "# boxplot\n",
    "data.boxplot('Speed_mph','Hour')\n",
    "plt.ylim([5,24]) # cut off outliers\n",
    "plt.ylabel('Speed (mph)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
